{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx44Pu/VbTrfawkUTEpMxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raoina/NLP-Learning-Journey/blob/main/NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z7A2t0Anuf7G"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATA = [\n",
        "    (\"Apple is looking at buying U.K. startup for $1 billion\", {\n",
        "        \"entities\": [(0, 5, \"ORG\"), (27, 31, \"LOC\")]\n",
        "    }),\n",
        "    (\"San Francisco considers banning sidewalk delivery robots\", {\n",
        "        \"entities\": [(0, 13, \"LOC\")]\n",
        "    }),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "    for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "import random\n",
        "\n",
        "optimizer = nlp.begin_training()\n",
        "for itn in range(10):\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "        nlp.update([example], drop=0.5, losses=losses)\n",
        "    print(losses)\n",
        "\n",
        "doc = nlp(\"Google is based in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L4u_gViukpp",
        "outputId": "198cb3f7-3219-483f-f2ea-9a15623a0e43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ner': np.float32(15.972323)}\n",
            "{'ner': np.float32(15.047707)}\n",
            "{'ner': np.float32(13.770542)}\n",
            "{'ner': np.float32(12.311717)}\n",
            "{'ner': np.float32(10.992633)}\n",
            "{'ner': np.float32(9.279618)}\n",
            "{'ner': np.float32(8.261411)}\n",
            "{'ner': np.float32(6.19745)}\n",
            "{'ner': np.float32(5.464408)}\n",
            "{'ner': np.float32(4.825523)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GvIw2YUIu8dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"eriktks/conll2003\")"
      ],
      "metadata": {
        "id": "6bs-wtnoutzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']"
      ],
      "metadata": {
        "id": "H1rmaoeauzV7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset['train'].features['ner_tags'].feature.names\n",
        "\n",
        "print(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueOEgD9ovQPb",
        "outputId": "af3bb937-5f51-4a75-ff6a-6c7992880fce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_spacy_format(dataset):\n",
        "    spacy_data = []\n",
        "\n",
        "    for example in dataset:\n",
        "        tokens = example['tokens']\n",
        "        tags = example['ner_tags']\n",
        "        text = ' '.join(tokens)\n",
        "        entities = []\n",
        "\n",
        "        offset = 0\n",
        "        for token, tag in zip(tokens, tags):\n",
        "            tag_name = labels[tag]\n",
        "            if tag_name != 'O':\n",
        "                start = text.find(token, offset)\n",
        "                end = start + len(token)\n",
        "                ent_type = tag_name.split('-')[-1]\n",
        "                entities.append((start, end, ent_type))\n",
        "                offset = end\n",
        "            else:\n",
        "                offset = text.find(token, offset) + len(token)\n",
        "\n",
        "        spacy_data.append((text, {'entities': entities}))\n",
        "\n",
        "    return spacy_data\n",
        "\n",
        "TRAIN_DATA = convert_to_spacy_format(train_dataset)\n"
      ],
      "metadata": {
        "id": "2zx8Nn4NvUVk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "for _, annotations in TRAIN_DATA:\n",
        "    for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "\n",
        "db = DocBin()\n",
        "for text, annot in tqdm(TRAIN_DATA):\n",
        "    doc = nlp.make_doc(text)\n",
        "    ents = []\n",
        "    for start, end, label in annot[\"entities\"]:\n",
        "        span = doc.char_span(start, end, label=label)\n",
        "        if span is not None:\n",
        "            ents.append(span)\n",
        "    doc.ents = ents\n",
        "    db.add(doc)\n",
        "\n",
        "db.to_disk(\"./train.spacy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksof-kXLvXRG",
        "outputId": "f2909390-1ebb-419f-85cf-a1175984b1a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14041/14041 [00:03<00:00, 4041.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init config config.cfg --lang en --pipeline ner"
      ],
      "metadata": {
        "id": "ULs934vQvu5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9TYJCgfvyzb",
        "outputId": "1883ccb1-fe12-4820-99e7-f9f8c566c7d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00     46.28    0.00    0.00    0.00    0.00\n",
            "  0     200         79.75   3317.64   62.25   64.83   59.87    0.62\n",
            "  0     400        276.63   2134.41   79.35   80.24   78.47    0.79\n",
            "  0     600        229.00   1927.59   85.12   85.77   84.49    0.85\n",
            "  0     800        404.25   1994.89   88.82   89.68   87.99    0.89\n",
            "  0    1000        301.74   2121.26   91.42   91.50   91.33    0.91\n",
            "  1    1200        364.63   2083.05   94.24   94.64   93.84    0.94\n",
            "  1    1400        373.89   1569.38   95.40   95.37   95.44    0.95\n",
            "  1    1600        443.72   1801.37   96.76   97.04   96.49    0.97\n",
            "  2    1800        510.78   1786.22   97.90   97.82   97.97    0.98\n",
            "  2    2000        584.55   1564.35   98.58   98.86   98.31    0.99\n",
            "  3    2200        582.42   1328.80   98.88   98.88   98.88    0.99\n",
            "  4    2400        726.23   1331.55   99.33   99.47   99.20    0.99\n",
            "  5    2600        739.19   1065.31   99.44   99.43   99.46    0.99\n",
            "  6    2800        667.92    885.94   99.59   99.63   99.55    1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp_ner = spacy.load(\"./output/model-best\")\n",
        "\n",
        "doc = nlp_ner(\"I am Rowaina Reda, I graduated form Computer and data Science faculty. I learn NLP course in Alexandria\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiWr-UEvv2TL",
        "outputId": "f0e71430-d21d-4145-eb8b-33ea98791f86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rowaina PER\n",
            "Reda PER\n",
            "Computer LOC\n",
            "NLP ORG\n",
            "Alexandria LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koTh9NO4ynIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}