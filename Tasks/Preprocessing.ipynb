{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0db01f",
   "metadata": {},
   "source": [
    "# Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563d05b",
   "metadata": {},
   "source": [
    "1. Lowercase Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbb6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world! this is an example.\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "text = \"Hello World! This is an Example.\"\n",
    "lowercase_text = lowercase_text(text)\n",
    "print(lowercase_text)  # hello world! this is an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff375ac8",
   "metadata": {},
   "source": [
    "2. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae7dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'example']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "tokens = ['hello', 'world', 'this', 'is','The', 'an', 'example']\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(filtered_tokens)  # ['hello', 'world', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9724",
   "metadata": {},
   "source": [
    "3. Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is an example with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "text = \"Hello, world! This is an example: with punctuation.\"\n",
    "clean_text = remove_punctuation(text)\n",
    "print(clean_text)  # \"Hello world This is an example with punctuation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0b2ae",
   "metadata": {},
   "source": [
    "4. Regular Expressions (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e49846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "# Match Simple Text\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Python is fun\"\n",
    "match = re.search(\"Python\", text)\n",
    "print(match.group())  # Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5dd6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "amazing\n"
     ]
    }
   ],
   "source": [
    "# Match Beginning and End\n",
    "\n",
    "# ^ matches start of string, $ matches end\n",
    "text = \"Python is amazing\"\n",
    "start_match = re.search(\"^Python\", text)\n",
    "print(start_match.group())  # Python\n",
    "\n",
    "end_match = re.search(\"amazing$\", text)\n",
    "print(end_match.group())  # amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2052a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '3', '5']\n",
      "['3', '35']\n"
     ]
    }
   ],
   "source": [
    "# Match Digits\n",
    "\n",
    "text = \"I have 3 apples and 35 oranges\"\n",
    "digits = re.findall(r\"\\d\", text)  # r prefix creates a raw string\n",
    "print(digits)  # ['3', '5']\n",
    "\n",
    "text = \"I have 3 apples and 35 oranges\"\n",
    "# \\d+ matches one or more digits\n",
    "numbers = re.findall(r\"\\d+\", text)\n",
    "print(numbers)  # ['3', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea881a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_123', 'has', 'logged', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Match Word Characters\n",
    "\n",
    "text = \"user_123 has logged in\"\n",
    "# \\w matches alphanumeric + underscore\n",
    "word_chars = re.findall(r\"\\w+\", text)\n",
    "print(word_chars)  # ['user_123', 'has', 'logged', 'in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a8524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'colour']\n"
     ]
    }
   ],
   "source": [
    "# matching zero or more \n",
    "\n",
    "text = \"color colour colouur\"\n",
    "pattern = re.findall(r\"colou?r\", text)  # ? means 0 or 1 of previous character\n",
    "print(pattern)  # ['color', 'colour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa36dce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loooove']\n"
     ]
    }
   ],
   "source": [
    "# Match One or More\n",
    "\n",
    "text = \"I loooove Python\"\n",
    "pattern = re.findall(r\"lo+ve\", text)  # + means 1 or more of previous character\n",
    "print(pattern)  # ['loooove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b81a9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['555-1234', '678-5678']\n"
     ]
    }
   ],
   "source": [
    "# Match Exact Number\n",
    "\n",
    "\n",
    "text = \"Phone numbers: 555-1234 and 555678-5678\"\n",
    "pattern = re.findall(r\"\\d{3}-\\d{4}\", text)  # {n} means exactly n occurrences\n",
    "print(pattern)  # ['555-1234', '555-5678']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a484cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'rat']\n"
     ]
    }
   ],
   "source": [
    "# Match Any of Several Characters\n",
    "\n",
    "text = \"The cat and the rat sat on the mat\"\n",
    "pattern = re.findall(r\"[cr]at\", text)  # matches 'cat' or 'rat'\n",
    "print(pattern)  # ['cat', 'rat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb0bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "['D', 'E']\n",
      "['a', '1', 'b', '2', 'c', '3', 'D', '4', 'E', '5']\n"
     ]
    }
   ],
   "source": [
    "# Match Range of Characters\n",
    "\n",
    "text = \"a1b2c3D4E5\"\n",
    "letters = re.findall(r\"[a-z]\", text)  # lowercase letters\n",
    "print(letters)  # ['a', 'b', 'c']\n",
    "\n",
    "uppercase = re.findall(r\"[A-Z]\", text)  # uppercase letters\n",
    "print(uppercase)  # ['D', 'E']\n",
    "\n",
    "alphanumeric = re.findall(r\"[a-zA-Z0-9]\", text)  # all alphanumeric\n",
    "print(alphanumeric)  # ['a', '1', 'b', '2', 'c', '3', 'D', '4', 'E', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd32b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user@example.com is valid\n",
      "invalid@email is invalid\n",
      "name.last@domain.co.uk is valid\n"
     ]
    }
   ],
   "source": [
    "# Email Validation\n",
    "\n",
    "emails = [\"user@example.com\", \"invalid@email\", \"name.last@domain.co.uk\"]\n",
    "pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "for email in emails:\n",
    "    if re.match(pattern, email):\n",
    "        print(f\"{email} is valid\")\n",
    "    else:\n",
    "        print(f\"{email} is invalid\")\n",
    "# user@example.com is valid\n",
    "# invalid@email is invalid\n",
    "# name.last@domain.co.uk is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8e73f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  apples and  oranges.\n",
      "There are NUM apples and NUM oranges.\n"
     ]
    }
   ],
   "source": [
    "# Number Removal/Normalization\n",
    "# /d is for digits\n",
    "import re\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def replace_numbers(text, replacement='NUM'):\n",
    "    return re.sub(r'\\d+', replacement, text)\n",
    "\n",
    "text = \"There are 123 apples and 456 oranges.\"\n",
    "text_no_numbers = remove_numbers(text)\n",
    "text_normalized = replace_numbers(text)\n",
    "\n",
    "print(text_no_numbers)  # \"There are  apples and  oranges.\"\n",
    "print(text_normalized)  # \"There are NUM apples and NUM oranges.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a85c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters unicode like  should be removed\n"
     ]
    }
   ],
   "source": [
    "# Noise Removal\n",
    "import re\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove special characters and symbols\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove ASCII/Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = \"Special @#! characters & unicode like 你好 should be removed.\"\n",
    "clean_text = remove_noise(text)\n",
    "print(clean_text)  # \"Special characters  unicode like  should be removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact us at [EMAIL] or visit [URL] or call [PHONE]\n"
     ]
    }
   ],
   "source": [
    "# Text Normalization with REGEX\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace URLs\n",
    "    #\\S means \"non-whitespace character\" (the opposite of \\s)\n",
    "    # \\s = whitespace (spaces, tabs, newlines)\n",
    "    # \\S = any character that is NOT whitespace\n",
    "\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "    \n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "    \n",
    "    # Replace phone numbers\n",
    "    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n",
    "    \n",
    "    # Replace multiple whitespaces with single\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Replace elongated words (e.g., \"hellooooo\" -> \"hello\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "text = \"Contact us at example@gmail.com or visit https://example.com or call 123-456-7890\"\n",
    "normalized_text = normalize_text(text)\n",
    "print(normalized_text)  # \"contact us at [EMAIL] or visit [URL] or call [PHONE]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62580390",
   "metadata": {},
   "source": [
    "5. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ca099",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller pieces, called tokens.\n",
    "These tokens can be:\n",
    "\n",
    "Words → Word-level tokenization\n",
    "\n",
    "Characters → Character-level tokenization\n",
    "\n",
    "Subwords → Subword-level tokenization (used in models like BERT, GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bce6f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'How', 'are', 'you', 'today', '?']\n",
      "['Hello world.', 'How are you today?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Using NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Word tokenization\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    # Sentence tokenization\n",
    "    sentence_tokens = nltk.sent_tokenize(text)\n",
    "    return word_tokens, sentence_tokens\n",
    "\n",
    "text = \"Hello world. How are you today?\"\n",
    "word_tokens, sentence_tokens = tokenize_text(text)\n",
    "print(word_tokens)  # ['Hello', 'world', '.', 'How', 'are', 'you', 'today', '?']\n",
    "print(sentence_tokens)  # ['Hello world.', 'How are you today?']\n",
    "\n",
    "\n",
    "# we can look at letters as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12ab27",
   "metadata": {},
   "source": [
    "Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a0294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'how', 'are', 'you', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Hello world. How are you today?\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8ab40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Ġworld', '.', 'ĠHow', 'Ġare', 'Ġyou', 'Ġtoday', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"Hello world. How are you today?\")\n",
    "print(tokens)\n",
    "\n",
    "# The Ġ symbol represents a space before the word (tokenized using byte-level BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe6c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', 'bel', 'iev', 'ability']\n",
      "['ban', 'ana', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"unbelievability\")\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(\"banana \")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5bcfd",
   "metadata": {},
   "source": [
    "6. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930bbb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter: ['run', 'run', 'ran', 'easili', 'fairli']\n",
      "Lancaster: ['run', 'run', 'ran', 'easy', 'fair']\n",
      "Snowball: ['run', 'run', 'ran', 'easili', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "def stem_words(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    snowball = SnowballStemmer('english')\n",
    "    \n",
    "    porter_stems = [porter.stem(token) for token in tokens]\n",
    "    lancaster_stems = [lancaster.stem(token) for token in tokens]\n",
    "    snowball_stems = [snowball.stem(token) for token in tokens]\n",
    "    \n",
    "    return porter_stems, lancaster_stems, snowball_stems\n",
    "\n",
    "tokens = ['running', 'runs', 'ran', 'easily', 'fairly']\n",
    "porter_stems, lancaster_stems, snowball_stems = stem_words(tokens)\n",
    "print(f\"Porter: {porter_stems}\")    # ['run', 'run', 'ran', 'easili', 'fairli']\n",
    "print(f\"Lancaster: {lancaster_stems}\")  # ['run', 'run', 'ran', 'easy', 'fair']\n",
    "print(f\"Snowball: {snowball_stems}\")    # ['run', 'run', 'ran', 'easili', 'fair']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86d4ed",
   "metadata": {},
   "source": [
    "7. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd16445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'run', 'ran', 'better', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "tokens = ['running', 'runs', 'ran', 'better', 'mice']\n",
    "lemmatized_tokens = lemmatize_words(tokens)\n",
    "print(lemmatized_tokens)  # ['running', 'run', 'ran', 'better', 'mouse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f4cf1",
   "metadata": {},
   "source": [
    "8. Spell Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2adb167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'world', 'example']\n"
     ]
    }
   ],
   "source": [
    "# pip install pyspellchecker spellchecker\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    spell = SpellChecker()\n",
    "    corrected = [spell.correction(token) for token in tokens]\n",
    "    return corrected\n",
    "\n",
    "tokens = ['helo', 'wrld', 'example']\n",
    "corrected_tokens = correct_spelling(tokens)\n",
    "print(corrected_tokens)  # ['hello', 'world', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eaff4a",
   "metadata": {},
   "source": [
    "9. Text Normalization with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb885728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install textblob\n",
    "# python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def normalize_with_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Correct spelling\n",
    "    corrected = blob.correct()\n",
    "    \n",
    "    # Get sentiment\n",
    "    sentiment = blob.sentiment\n",
    "    \n",
    "    # Get noun phrases\n",
    "    noun_phrases = blob.noun_phrases\n",
    "    \n",
    "    return str(corrected), sentiment, noun_phrases\n",
    "\n",
    "text = \"The quik brown fox jumpd over the lazzy dog.\"\n",
    "# text = \"guuod feeling\"\n",
    "corrected, sentiment, noun_phrases = normalize_with_textblob(text)\n",
    "\n",
    "print(f\"Corrected: {corrected}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Noun phrases: {noun_phrases}\")\n",
    "\n",
    "# Corrected: The quick brown fox jumped over the lazy dog.\n",
    "# Sentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
    "# Noun phrases: ['brown fox jumpd', 'lazzy dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f09875",
   "metadata": {},
   "source": [
    "10. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa29d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "# pip intall spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "def extract_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "entities = extract_entities(text)\n",
    "print(entities)  # [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78bd2a",
   "metadata": {},
   "source": [
    "11. Text Cleaning (HTML/XML tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b466396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is sample HTML text?.\n",
      "This is sample HTML text?.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html_text):\n",
    "    # Using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    clean_text = soup.get_text(separator=\" \", strip=True)\n",
    "    return clean_text\n",
    "\n",
    "def clean_html_regex(html_text):\n",
    "    # Using regex\n",
    "    clean_text = re.sub(r'<.*?>', '', html_text)\n",
    "    return clean_text\n",
    "\n",
    "html = \"<div><p>This is <b>sample</b> HTML text?.</p></div>\"\n",
    "clean_bs = clean_html(html)\n",
    "clean_re = clean_html_regex(html)\n",
    "\n",
    "print(clean_bs)  # \"This is sample HTML text.\"\n",
    "print(clean_re)  # \"This is sample HTML text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24af3bd",
   "metadata": {},
   "source": [
    "12. Contractions Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1559017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot do this and I will not try it.\n"
     ]
    }
   ],
   "source": [
    "# pip install contractions\n",
    "import contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "text = \"I can't do this and I won't try it.\"\n",
    "expanded = expand_contractions(text)\n",
    "print(expanded)  # \"I cannot do this and I will not try it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f29bb",
   "metadata": {},
   "source": [
    "13. Text Vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW features: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "BoW matrix:\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]]\n",
      "\n",
      "TF-IDF features: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "TF-IDF matrix:\n",
      " [[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def vectorize_texts(texts):\n",
    "    # Bag of Words\n",
    "    count_vec = CountVectorizer()\n",
    "    bow = count_vec.fit_transform(texts)\n",
    "    \n",
    "    # TF-IDF\n",
    "    tfidf_vec = TfidfVectorizer()\n",
    "    tfidf = tfidf_vec.fit_transform(texts)\n",
    "    \n",
    "    return bow, count_vec.get_feature_names_out(), tfidf, tfidf_vec.get_feature_names_out()\n",
    "\n",
    "texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "]\n",
    "\n",
    "bow, bow_features, tfidf, tfidf_features = vectorize_texts(texts)\n",
    "print(\"BoW features:\", bow_features)\n",
    "print(\"BoW matrix:\\n\", bow.toarray())\n",
    "print(\"\\nTF-IDF features:\", tfidf_features)\n",
    "print(\"TF-IDF matrix:\\n\", tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1d3b1",
   "metadata": {},
   "source": [
    "14. Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim\n",
    "# pip install --upgrade --force-reinstall --no-cache-dir numpy gensim\n",
    "# restart kernel\n",
    "import gensim.downloader as api\n",
    "\n",
    "def get_word_embeddings(words):\n",
    "    # Load pre-trained Word2Vec embeddings\n",
    "    model = api.load(\"word2vec-google-news-300\")\n",
    "    \n",
    "    embeddings = {}\n",
    "    for word in words:\n",
    "            embeddings[word] = model[word]\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\"]\n",
    "embeddings = get_word_embeddings(words)\n",
    "\n",
    "for word, vector in embeddings.items():\n",
    "    print(f\"{word}: vector shape {vector.shape}\")\n",
    "\n",
    "# king: vector shape (300,)\n",
    "# queen: vector shape (300,)\n",
    "# man: vector shape (300,)\n",
    "# woman: vector shape (300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb02608",
   "metadata": {},
   "source": [
    "18. Language Detection and Translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googletrans==4.0.0rc1 -q\n",
    "# !pip install langdetect -q\n",
    "# restart kernel\n",
    "\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "def detect_and_translate(text, target_lang='en'):\n",
    "    # Detect language\n",
    "    source_lang = detect(text)\n",
    "    \n",
    "    # Translate text\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, src=source_lang, dest=target_lang)\n",
    "    \n",
    "    return source_lang, translation.text\n",
    "\n",
    "text = \"Bonjour le monde\"\n",
    "source, translation = detect_and_translate(text)\n",
    "print(f\"Detected language: {source}\")\n",
    "print(f\"Translation: {translation}\")\n",
    "\n",
    "# Detected language: fr\n",
    "# Translation: Hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764425",
   "metadata": {},
   "source": [
    "19. Custom Vocabulary Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742d718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'this': 4, 'is': 4, 'the': 4, 'document': 4, '.': 3, 'first': 2}\n",
      "Token to ID mapping: {'this': 0, 'is': 1, 'the': 2, 'document': 3, '.': 4, 'first': 5}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(texts, min_freq=2, max_vocab_size=10000):\n",
    "    # Tokenize all texts\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count frequency\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Filter by frequency and vocabulary size\n",
    "    vocab = {token: count for token, count in token_counts.most_common(max_vocab_size) \n",
    "             if count >= min_freq}\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    token2id = {token: idx for idx, (token, _) in enumerate(vocab.items())}\n",
    "    id2token = {idx: token for token, idx in token2id.items()}\n",
    "    \n",
    "    return vocab, token2id, id2token\n",
    "\n",
    "texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "vocab, token2id, id2token = create_vocabulary(texts, min_freq=2)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Token to ID mapping:\", token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac3c69",
   "metadata": {},
   "source": [
    "Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98771f4",
   "metadata": {},
   "source": [
    "20. Comprehensive Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5102de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'preprocessing', 'pipeline', 'remove', 'punctuation', 'number', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet'])\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "text = \"This is an example! The preprocessing pipeline removes punctuation, numbers (123), and stopwords.\"\n",
    "tokens = preprocess_text(text)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
